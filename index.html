<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation</title>

  <!-- Meta tags -->
  <meta name="google-site-verification" content="JWoNA1uZW6yUOOmWuXfBl6u6ZkCAy1a5m2j8Qq8GZfc" />
  <meta name="keywords" content="Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation">
  <meta property="og:site_name" content="Pose Anything">
  <meta property="og:url" content="">
  <meta property="og:title" content="Pose Anything">
  <meta name="description"
    content="A Graph-Based Approach for Category-Agnostic Pose Estimation">
  <meta property="og:description"
    content="A Graph-Based Approach for Category-Agnostic Pose Estimation">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css" />
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css" />
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script type="text/javascript" src="./static/slick/slick.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script type="text/javascript" src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js"></script> -->
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Graph-Based Approach for Category-Agnostic Pose Estimation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://orhir.github.io">Or Hirschorn</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.co.il/citations?hl=iw&user=hpItE1QAAAAJ">Shai Avidan</a>
              </span>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Tel Aviv University</span>
            </div>

            <!-- <div>
              <p style="font-size:23px;font-weight:bold;padding-top: 5px;">CVPR 2023</p>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link.
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>PDF</span>
                  </a>
                </span>
                -->
                <!-- arXiv Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.17891.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/orhir/PoseAnything"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Data Link. -->
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1uRyGB-P5Tc_6TmAZ6RnOi0SWjGq9b28T?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google-drive"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=gJNas6F-BJo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div id="teaser" class="has-text-centered">
          <img style="width: 60%;" src="./static/images/Pose_Anything_Teaser.png" alt="Pose Anything teaser.">
        </div>

<!--        <video id="teaser" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">-->
<!--          <source src="static/videos/the_chosen_one_teaser.mp4" type="video/mp4">-->
<!--        </video>-->

        <h2 class="subtitle has-text-centered">
          <span class="method-name"></span>Given only <b><i>one</i></b> example image and skeleton, our method
          can perform pose estimation on unseen categories.
        </h2>
      </div>
    </div>
  </section>

  <!-- Abstract. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
Traditional 2D pose estimation models are limited by their category-specific design, making them suitable only for predefined object categories. This restriction becomes particularly challenging when dealing with novel objects due to the lack of relevant training data. To address this limitation, category-agnostic pose estimation (CAPE) was introduced. CAPE aims to enable keypoint localization for arbitrary object categories using a few-shot single model, requiring minimal support images with annotated keypoints.
We present a significant departure from conventional CAPE techniques, which treat keypoints as isolated entities, by treating the input pose data as a graph. We leverage the inherent geometrical relations between keypoints through a graph-based network to break symmetry, preserve structure, and better handle occlusions. We validate our approach on the MP-100 benchmark, a comprehensive dataset comprising over 20,000 images spanning over 100 categories. Our solution boosts performance by 0.98% under a 1-shot setting, achieving a new state-of-the-art for CAPE. Additionally, we enhance the dataset with skeleton annotations.
Our code and data are publicly available.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/gJNas6F-BJo?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  <!-- Paper video. -->
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <div class="content has-text-justified">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Video</h2>
                <div class="publication-video">
                  <iframe src="https://www.youtube.com/embed/VlieNoCwHO4?rel=0&amp;showinfo=0" frameborder="0"
                    allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <!-- Method explanation -->
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <h2 class="title is-3">Method</h2>

          <div class="content has-text-justified">
            <div class="has-text-centered">
              <img src="./static/images/arch.png" />
              <br />
            </div>
            <p>
The core idea of our work is to take advantage of the geometrical structure encoded in the pose graph. Our method is built upon the enhanced baseline, replacing the transformer decoder module with our novel Graph Transformer Decoder.
            </p>

            <p>
              We recognize that self-attention, a mechanism that helps our model focus on relevant information, can be thought of as a graph convolutional network (GCN) with a learnable adjacency matrix. When we are dealing with pose estimation for a single category, this mechanism is sufficient for learning the relationships between keypoints and integrating a learned structure into the model.
However, for category-agnostic pose estimation (CAPE) tasks, where the model needs to work with various object categories, it is beneficial to explicitly consider the semantic connections between keypoints. This helps the model break symmetry, maintain consistent structure, and handle noisy keypoints by sharing information among neighboring keypoints.
We implemented this prior in the transformer decoder. Specifically, GTD is based on the original CapeFormer decoder, changing the feed-forward network from a simple MLP to a GCN network. To address the potential problem of excessive smoothing often observed in deep GCNs, which can lead to a reduction in the distinctiveness of node characteristics and consequently a decline in performance, we introduce a linear layer for each node following the GCN layer.
            </p>



          </div>
        </div>
      </div>
    </div>
  </section>
-->
  <!-- General Examples -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <h2 class="title is-3">Results</h2>
          <h3 class="title is-4">Qualitative Results</h3>
          <div class="content has-text-justified">
            <p>
              Using our method, given a <b>support</b> image and skeleton we can perform structure-consistent pose estimation on images from unseen categories.
            </p>
          </div>

          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/23_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/23_1_GT.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/23_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/36_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/36_1_gt.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/36_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/50_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/50_1_gt.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/50_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/69_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/69_1_GT.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/69_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/84_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/84_1_gt.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/84_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/81_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/81_1_GT.png" />
                    <p class="caption"><b>GT</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/qualitative/81_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                </div>
              </div>

            </div>
          </section>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <h3 class="title is-4">Out-of-Distribution</h3>
          <div class="content has-text-justified">
            <p>
Our model, which was trained on real images only, demonstrates its adaptability and effectiveness across varying data sources such as cartoons and imaginary animals, created using a diffusion model. Furthermore, our model demonstrates satisfactory performance even when the <b>Support</b> and query images are from different domains.
            </p>
          </div>

          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="ood-carousel" class="carousel ood-carousel">
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/0_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/0_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                    <div class="item has-text-centered">
                    <img src="./static/images/ood/1_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/1_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                                      <div class="item has-text-centered">
                    <img src="./static/images/ood/2_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/2_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>



                                      <div class="item has-text-centered">
                    <img src="./static/images/ood/7_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/7_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>



                  <div class="item has-text-centered">
                    <img src="./static/images/ood/8_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/8_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/ood/9_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/9_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>

                  <div class="item has-text-centered">
                    <img src="./static/images/ood/11_0.png" />
                    <p class="caption"><b>Support</b></p>
                  </div>
                  <div class="item has-text-centered">
                    <img src="./static/images/ood/11_1.png" />
                    <p class="caption"><b>Ours</b></p>
                  </div>
                </div>
              </div>
            </div>
          </section>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <h3 class="title is-4">Quantitative Results</h3>
          <div class="content has-text-justified">
            <p>

              We compare our method with the previous CAPE methods CapeFormer and POMNet and three baselines:  ProtoNet, MAML, and Fine-tuned.

We report results on the MP-100 dataset under 1-shot and 5-shot settings. As can be seen, the enhanced baseline models, which are agnostic to the keypoints order as opposed to CapeFormer, outperform previous methods and improve the average PCK by 0.94% under the 1-shot setting and 1.60% under the 5-shot setting.
Our graph-based method further improves performance, improving the enhanced baseline by 1.22% under the 1-shot setting and 0.22% under the 5-shot setting, achieving new state-of-the-art results for both settings.
We also show the scalability of our design. Similar to DETR-based models, employing a larger backbone improves performance. We show that our graph decoder design also enhances the performance of the larger enhanced baseline, improving results by 1.02% and 0.34% under 1-shot and 5-shot settings respectively.

            </p>
                  <div class="item has-text-centered">
                    <img src="./static/images/quantitative.png" />
                  </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p>If you find this research useful, please cite the following:</p>

      <pre><code>@misc{hirschorn2023pose,
      title={Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation},
      author={Or Hirschorn and Shai Avidan},
      year={2023},
      eprint={2311.17891},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <p>
          This page was adapted from <a href="https://github.com/nerfies/nerfies.github.io">this</a> source code.
        </p>
      </div>
    </div>
  </footer>

</body>

</html>
